{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUWVNM0W8bQnGY1paHHFxX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samx178/Diabetes-Predictor-AIML_model/blob/main/ESE_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY5xEra_YpHY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PATH = \"train.csv\"\n",
        "TEST_PATH = \"test.csv\"\n",
        "\n",
        "TARGET_COL = \"quality_grade\"   # üëà kal bas ye badalna\n"
      ],
      "metadata": {
        "id": "DzQp99MnYzjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/kaggle/input/mle-ese-mock/train (5).csv\")\n",
        "test_df = pd.read_csv(\"/kaggle/input/mle-ese-mock/test (4).csv\")\n",
        "\n",
        "# Target me NaN rows hatao (mandatory)\n",
        "train_df = train_df.dropna(subset=[TARGET_COL])\n"
      ],
      "metadata": {
        "id": "pTM4KuJ1Y2Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_df.drop(columns=[TARGET_COL])\n",
        "y = train_df[TARGET_COL]\n"
      ],
      "metadata": {
        "id": "gA8XskRmY4DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n"
      ],
      "metadata": {
        "id": "SOTUNLTXY4rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n"
      ],
      "metadata": {
        "id": "y-mEifybY6Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PIPELINE 1 & 2 (RF + XGB)\n",
        "numeric_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", numeric_pipeline, numeric_cols),\n",
        "    (\"cat\", categorical_pipeline, categorical_cols)\n",
        "])\n"
      ],
      "metadata": {
        "id": "c-MQ6_xuY8dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pipeline 1 random forest\n",
        "rf_pipeline = Pipeline([\n",
        "    (\"preprocessing\", preprocessor),\n",
        "    (\"model\", RandomForestClassifier(\n",
        "        n_estimators=400,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n"
      ],
      "metadata": {
        "id": "56XOmnftY_vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pipeline XGboost\n",
        "xgb_pipeline = Pipeline([\n",
        "    (\"preprocessing\", preprocessor),\n",
        "    (\"model\", XGBClassifier(\n",
        "        n_estimators=500,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        eval_metric=\"mlogloss\",\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n"
      ],
      "metadata": {
        "id": "MiMtlvvVZBZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_cb = X.copy()\n",
        "test_cb = test_df.copy()\n",
        "\n",
        "for col in categorical_cols:\n",
        "    X_cb[col] = X_cb[col].fillna(\"missing\")\n",
        "    test_cb[col] = test_cb[col].fillna(\"missing\")\n",
        "\n",
        "for col in numeric_cols:\n",
        "    median = X_cb[col].median()\n",
        "    X_cb[col] = X_cb[col].fillna(median)\n",
        "    test_cb[col] = test_cb[col].fillna(median)\n"
      ],
      "metadata": {
        "id": "TrJOWrpeZCKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_features = [X_cb.columns.get_loc(col) for col in categorical_cols]\n"
      ],
      "metadata": {
        "id": "gurepRJYZGf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_pipeline = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    depth=6,\n",
        "    learning_rate=0.05,\n",
        "    loss_function=\"MultiClass\",\n",
        "    verbose=0,\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "0PRH10YDZNJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CatBoost log loss best hota hai\n",
        "# XGB accuracy best hoti hai\n",
        "MODEL_TYPE = \"catboost\"\n",
        "# MODEL_TYPE = \"rf\"\n",
        "# MODEL_TYPE = \"gb\"\n",
        "# MODEL_TYPE = \"xgb\"\n"
      ],
      "metadata": {
        "id": "HREkYj4GZPhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MODEL_TYPE == \"catboost\":\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_cb, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        "    )\n",
        "\n",
        "    cat_pipeline.fit(X_train, y_train, cat_features=cat_features)\n",
        "\n",
        "    val_preds = cat_pipeline.predict(X_val)\n",
        "    val_proba = cat_pipeline.predict_proba(X_val)\n",
        "\n",
        "    print(\"Validation Accuracy:\", accuracy_score(y_val, val_preds))\n",
        "    print(\"Validation Log Loss:\", log_loss(y_val, val_proba))\n",
        "\n",
        "    cat_pipeline.fit(X_cb, y_encoded, cat_features=cat_features)\n",
        "    test_preds = cat_pipeline.predict(test_cb)\n",
        "\n",
        "elif MODEL_TYPE == \"rf\":\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        "    )\n",
        "\n",
        "    rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    val_preds = rf_pipeline.predict(X_val)\n",
        "    val_proba = rf_pipeline.predict_proba(X_val)\n",
        "\n",
        "    print(\"Validation Accuracy:\", accuracy_score(y_val, val_preds))\n",
        "    print(\"Validation Log Loss:\", log_loss(y_val, val_proba))\n",
        "\n",
        "    rf_pipeline.fit(X, y_encoded)\n",
        "    test_preds = rf_pipeline.predict(test_df)\n",
        "\n",
        "elif MODEL_TYPE == \"gb\":\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        "    )\n",
        "\n",
        "    gb_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    val_preds = gb_pipeline.predict(X_val)\n",
        "    val_proba = gb_pipeline.predict_proba(X_val)\n",
        "\n",
        "    print(\"Validation Accuracy:\", accuracy_score(y_val, val_preds))\n",
        "    print(\"Validation Log Loss:\", log_loss(y_val, val_proba))\n",
        "\n",
        "    gb_pipeline.fit(X, y_encoded)\n",
        "    test_preds = gb_pipeline.predict(test_df)\n",
        "\n",
        "elif MODEL_TYPE == \"xgb\":\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        "    )\n",
        "\n",
        "    xgb_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    val_preds = xgb_pipeline.predict(X_val)\n",
        "    val_proba = xgb_pipeline.predict_proba(X_val)\n",
        "\n",
        "    print(\"Validation Accuracy:\", accuracy_score(y_val, val_preds))\n",
        "    print(\"Validation Log Loss:\", log_loss(y_val, val_proba))\n",
        "\n",
        "    xgb_pipeline.fit(X, y_encoded)\n",
        "    test_preds = xgb_pipeline.predict(test_df)\n"
      ],
      "metadata": {
        "id": "9dbmU4BuZRN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss, r2_score\n"
      ],
      "metadata": {
        "id": "QJ4Vxd4-ZTru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_pipeline.fit(X_train, y_train, cat_features=cat_features)\n",
        "\n",
        "# Predictions\n",
        "val_preds = cat_pipeline.predict(X_val)\n",
        "val_proba = cat_pipeline.predict_proba(X_val)\n",
        "\n",
        "print(\"Validation Accuracy:\", accuracy_score(y_val, val_preds))\n",
        "print(\"Validation Log Loss:\", log_loss(y_val, val_proba))\n"
      ],
      "metadata": {
        "id": "erbc9IhsZUXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sub = pd.read_csv(\"/kaggle/input/mle-ese-mock/submission (6).csv\")\n",
        "print(sample_sub.columns)\n"
      ],
      "metadata": {
        "id": "azwYiduhZXUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_proba = cat_pipeline.predict_proba(test_cb)\n"
      ],
      "metadata": {
        "id": "egkQ31acZZKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame()\n",
        "\n",
        "# id column\n",
        "submission[\"id\"] = sample_sub[\"id\"]\n",
        "\n",
        "# class columns with Status_ prefix\n",
        "class_columns = sample_sub.columns[1:]   # Q1_..., Q2_...\n",
        "\n",
        "for i, col in enumerate(class_columns):\n",
        "    submission[f\"Status_{col}\"] = test_proba[:, i]\n"
      ],
      "metadata": {
        "id": "G5r1DyIvZbhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv(\"submission5.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "oY06AOb2Zcgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(submission.head())\n",
        "print(submission.columns)\n"
      ],
      "metadata": {
        "id": "6QERi_KRZedP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "submission_new = pd.read_csv(\"submission4.csv\")\n",
        "\n",
        "print(submission_new)\n"
      ],
      "metadata": {
        "id": "hz9tVB81Zg53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(submission_new.head())\n"
      ],
      "metadata": {
        "id": "Cpx2ndluZixU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(submission_new.shape)\n"
      ],
      "metadata": {
        "id": "vAYFApgxZkMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(submission_new.columns)\n"
      ],
      "metadata": {
        "id": "qGhm3lSaZl5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CSV trick 1"
      ],
      "metadata": {
        "id": "TcFtgJUCZoFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample_sub = pd.read_csv(\"submission (6).csv\")\n"
      ],
      "metadata": {
        "id": "G08Mppz7ZsZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(sample_sub.columns)\n"
      ],
      "metadata": {
        "id": "z2CIjw78Ztyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_proba = model.predict_proba(test_data)\n"
      ],
      "metadata": {
        "id": "W-lk4sEkZvD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# submission = sample_sub.copy()\n",
        "\n",
        "# # id column exactly as sample\n",
        "# submission.iloc[:, 0] = sample_sub.iloc[:, 0]\n",
        "\n",
        "# # baaki columns order-wise fill\n",
        "# for i in range(1, submission.shape[1]):\n",
        "#     submission.iloc[:, i] = test_proba[:, i-1]\n",
        "\n",
        "# submission.to_csv(\"final_submission.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "KHQrU_oLZwxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(submission.head())\n",
        "# print(submission.columns)\n",
        "# print(submission.shape)\n"
      ],
      "metadata": {
        "id": "STYM-s3OZyTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trick 2"
      ],
      "metadata": {
        "id": "Re4AMANqZzyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# def build_submission_csv(\n",
        "#     sample_submission_path,\n",
        "#     test_proba,\n",
        "#     output_filename=\"submission.csv\"\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     Universal Kaggle submission builder.\n",
        "#     Works for any dataset as long as sample submission is correct.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # 1. Read sample submission (single source of truth)\n",
        "#     sample_sub = pd.read_csv(sample_submission_path)\n",
        "\n",
        "#     # 2. Basic validation\n",
        "#     if test_proba.shape[1] != (sample_sub.shape[1] - 1):\n",
        "#         raise ValueError(\n",
        "#             f\"Class count mismatch: \"\n",
        "#             f\"model={test_proba.shape[1]}, \"\n",
        "#             f\"sample_submission={sample_sub.shape[1] - 1}\"\n",
        "#         )\n",
        "\n",
        "#     # 3. Clone sample submission\n",
        "#     submission = sample_sub.copy()\n",
        "\n",
        "#     # 4. Keep id column exactly same\n",
        "#     submission.iloc[:, 0] = sample_sub.iloc[:, 0]\n",
        "\n",
        "#     # 5. Fill all remaining columns order-wise\n",
        "#     for i in range(1, submission.shape[1]):\n",
        "#         submission.iloc[:, i] = test_proba[:, i - 1]\n",
        "\n",
        "#     # 6. Save CSV\n",
        "#     submission.to_csv(output_filename, index=False)\n",
        "\n",
        "#     print(f\"‚úÖ Submission saved as: {output_filename}\")\n",
        "#     print(\"üìå Columns used:\", list(submission.columns))\n",
        "\n",
        "#     return submission\n"
      ],
      "metadata": {
        "id": "ryYhb2KtZ0mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_proba = cat_model.predict_proba(test_cb)\n"
      ],
      "metadata": {
        "id": "C5syye8iZ3eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# submission = build_submission_csv(\n",
        "#     sample_submission_path=\"/kaggle/input/mle-ese-mock/submission (6).csv\",\n",
        "#     test_proba=test_proba,\n",
        "#     output_filename=\"submission4.csv\"\n",
        "# )\n"
      ],
      "metadata": {
        "id": "_9Z8fBIEZ4vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#raj ka code"
      ],
      "metadata": {
        "id": "SLC1BncDZ5aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    log_loss,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Ipx4HxYJZ8D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# GLOBAL WARNING CONTROL (SAFE & CLEAN)\n",
        "# ==========================================\n",
        "import warnings\n",
        "\n",
        "# 1Ô∏è‚É£ Ignore known, harmless FutureWarnings (seaborn / pandas)\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    category=FutureWarning\n",
        ")\n",
        "\n",
        "# 2Ô∏è‚É£ Ignore pandas RuntimeWarnings from NaN comparisons\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    category=RuntimeWarning,\n",
        "    module=\"pandas\"\n",
        ")\n",
        "\n",
        "# 3Ô∏è‚É£ Ignore seaborn warnings (visualization only)\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    module=\"seaborn\"\n",
        ")\n",
        "\n",
        "# 4Ô∏è‚É£ Safety: ensure numpy doesn't spam invalid comparisons\n",
        "np.seterr(invalid='ignore')"
      ],
      "metadata": {
        "id": "iPXrONblZ8qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=pd.read_csv(\"/kaggle/input/final-everything/train.csv\")\n",
        "test=pd.read_csv(\"/kaggle/input/final-everything/test.csv\")"
      ],
      "metadata": {
        "id": "rfrs4ICbaIkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.isnull().sum()"
      ],
      "metadata": {
        "id": "4Vu9RG0JaKhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.isnull().sum()"
      ],
      "metadata": {
        "id": "d3asQLk_aNJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.dropna(subset=['yaha pe targeted column kar de lawde']) #output label"
      ],
      "metadata": {
        "id": "hHjDzqJaaO8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_id=test['id']\n",
        "test=test.drop(columns=['id'])"
      ],
      "metadata": {
        "id": "rIDtjIE-aQY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=train.drop(columns=['target column'])\n",
        "y=train['target column']"
      ],
      "metadata": {
        "id": "H_NDPxJjaR0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)"
      ],
      "metadata": {
        "id": "IUcfnjqLaS_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features=X.select_dtypes(include=['int64','float64']).columns\n",
        "categorical_features=X.select_dtypes(include=['object','category']).columns"
      ],
      "metadata": {
        "id": "h6z5QfGKaUtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_pipeline=Pipeline(steps=[\n",
        "    ('impute',SimpleImputer(strategy='mean')),\n",
        "    ('scaler',StandardScaler())\n",
        "])\n",
        "categorical_pipeline=Pipeline(steps=[\n",
        "    ('impute',SimpleImputer(strategy='most_frequent')),\n",
        "    ('encode',OneHotEncoder(handle_unknown='ignore'))\n",
        "])"
      ],
      "metadata": {
        "id": "iQfG-RZuaW-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#VISUALISAION\n",
        "# STEP 1: HISTPLOT\n",
        "# ==========================================\n",
        "print(\"Step 1: Histplots\")\n",
        "for col in numeric_features:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.histplot(X[col].dropna(), kde=True, color='royalblue')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TOHP9y2uaYdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 2: TARGET COUNTS\n",
        "# ==========================================\n",
        "print(\"\\nStep 2: Target Counts\")\n",
        "print(y.value_counts())\n",
        "sns.countplot(x=y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OfvyVurXabIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 3: BOXPLOT (Before)\n",
        "# ==========================================\n",
        "print(\"\\nStep 3: Boxplots (Before)\")\n",
        "for col in numeric_features:\n",
        "    plt.figure(figsize=(8, 2))\n",
        "    sns.boxplot(x=X[col], color='tomato')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-02WAUbDaa4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 4: OUTLIER & INF HANDLING (ROBUST VERSION)\n",
        "# ==========================================\n",
        "print(\"\\nStep 4: Handling Outliers & Converting Infinity to NaN\")\n",
        "\n",
        "# 1Ô∏è‚É£ Replace inf ‚Üí NaN\n",
        "X_train[numeric_features] = X_train[numeric_features].replace([np.inf, -np.inf], np.nan)\n",
        "X_test[numeric_features]  = X_test[numeric_features].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# 2Ô∏è‚É£ Compute IQR on TRAIN\n",
        "Q1 = X_train[numeric_features].quantile(0.25)\n",
        "Q3 = X_train[numeric_features].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# 3Ô∏è‚É£ Keep only valid columns (IQR > 0 and not NaN)\n",
        "valid_cols = IQR[(IQR > 0) & (~IQR.isna())].index\n",
        "\n",
        "# 4Ô∏è‚É£ Clip only valid columns\n",
        "lower = Q1[valid_cols] - 1.5 * IQR[valid_cols]\n",
        "upper = Q3[valid_cols] + 1.5 * IQR[valid_cols]\n",
        "\n",
        "X_train[valid_cols] = X_train[valid_cols].clip(lower, upper, axis=1)\n",
        "X_test[valid_cols]  = X_test[valid_cols].clip(lower, upper, axis=1)\n"
      ],
      "metadata": {
        "id": "xVGZBZKiafCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 5: RE-CHECK TARGET COUNTS\n",
        "# ==========================================\n",
        "print(f\"Total Unique Classes: {y.nunique()}\")\n",
        "print(\"-\" * 30)\n",
        "print(y.value_counts())"
      ],
      "metadata": {
        "id": "SMLhCcQPahu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 6: RE-CHECK BOXPLOTS (AFTER CLEANING)\n",
        "# ==========================================\n",
        "print(\"\\nStep 6: Final Visual Checks\")\n",
        "\n",
        "for col in numeric_features:\n",
        "    plt.figure(figsize=(8, 2))\n",
        "    sns.boxplot(x=X_train[col], color='limegreen')\n",
        "    plt.title(f\"Boxplot of {col}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "xoBXEIqGajKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 7: NORMAL PAIRPLOT\n",
        "# ==========================================\n",
        "print(\"\\nStep 7: Generating Normal Pairplot\")\n",
        "\n",
        "plot_df = pd.concat([X_train, y_train], axis=1).sample(\n",
        "    min(500, len(X_train)),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "sns.pairplot(plot_df, hue='target column')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Lpbg14t5alAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 8: HEATMAP (Fixed)\n",
        "# ==========================================\n",
        "print(\"\\nStep 8: Corrected Heatmap\")\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(X.corr(numeric_only=True), annot=True, cmap='RdYlBu', center=0, square=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NRtZByHtalni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "# model = LogisticRegression(\n",
        "#     C=1.0,\n",
        "#     penalty='l2',\n",
        "#     solver='lbfgs',\n",
        "#     max_iter=1000,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# model = RandomForestClassifier(\n",
        "#     n_estimators=400,\n",
        "#     max_depth=None,\n",
        "#     min_samples_split=5,\n",
        "#     min_samples_leaf=2,\n",
        "#     max_features='sqrt',\n",
        "#     random_state=42,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "model = GradientBoostingClassifier(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=4,\n",
        "    subsample=0.9,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# from sklearn.ensemble import AdaBoostClassifier\n",
        "# model = AdaBoostClassifier(\n",
        "#     n_estimators=300,\n",
        "#     learning_rate=0.05,\n",
        "#     random_state=42\n",
        "# )\n",
        "\n",
        "# from sklearn.ensemble import ExtraTreesClassifier\n",
        "# model = ExtraTreesClassifier(\n",
        "#     n_estimators=500,\n",
        "#     max_depth=None,\n",
        "#     min_samples_split=5,\n",
        "#     min_samples_leaf=2,\n",
        "#     max_features='sqrt',\n",
        "#     random_state=42,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "# from xgboost import XGBClassifier\n",
        "# model = XGBClassifier(\n",
        "#     n_estimators=600,\n",
        "#     learning_rate=0.05,\n",
        "#     max_depth=6,\n",
        "#     subsample=0.9,\n",
        "#     colsample_bytree=0.9,\n",
        "#     reg_lambda=1,\n",
        "#     objective='multi:softprob',\n",
        "#     eval_metric='mlogloss',\n",
        "#     tree_method='hist',\n",
        "#     random_state=42,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "# from lightgbm import LGBMClassifier\n",
        "# model = LGBMClassifier(\n",
        "#     n_estimators=600,\n",
        "#     learning_rate=0.05,\n",
        "#     max_depth=-1,\n",
        "#     num_leaves=63,\n",
        "#     subsample=0.9,\n",
        "#     colsample_bytree=0.9,\n",
        "#     random_state=42,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "# from catboost import CatBoostClassifier\n",
        "# model = CatBoostClassifier(\n",
        "#     iterations=600,\n",
        "#     learning_rate=0.05,\n",
        "#     depth=6,\n",
        "#     loss_function='MultiClass',\n",
        "#     random_seed=42,\n",
        "#     verbose=False\n",
        "# )\n"
      ],
      "metadata": {
        "id": "DmuY0VWAaoow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing=ColumnTransformer(transformers=[\n",
        "    ('num',numerical_pipeline,numeric_features),\n",
        "    ('cat',categorical_pipeline,categorical_features)\n",
        "])"
      ],
      "metadata": {
        "id": "K8VrwnLIaqu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline=Pipeline(steps=[\n",
        "    ('preprocessor',preprocessing),\n",
        "    ('model',model)\n",
        "])"
      ],
      "metadata": {
        "id": "6d-21fGAardx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)  # fit on train\n",
        "y_test_enc = le.transform(y_test)        # transform test"
      ],
      "metadata": {
        "id": "R9MWF4DYayRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict class labels\n",
        "y_pred = pipeline.predict(X_test)\n",
        "# Predict class probabilities (needed for log-loss, AUC, calibration)\n",
        "y_pred_proba = pipeline.predict_proba(X_test)"
      ],
      "metadata": {
        "id": "gUC_54hia0NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "acc = accuracy_score(y_test_enc, y_pred)\n",
        "\n",
        "# Log Loss\n",
        "ll = log_loss(y_test_enc, y_pred_proba)\n",
        "\n",
        "# Precision, Recall, F1 (weighted = handles class imbalance)\n",
        "prec = precision_score(y_test_enc, y_pred, average='weighted')\n",
        "rec  = recall_score(y_test_enc, y_pred, average='weighted')\n",
        "f1   = f1_score(y_test_enc, y_pred, average='weighted')\n",
        "\n",
        "print(\"Accuracy :\", acc)\n",
        "print(\"Log Loss :\", ll)\n",
        "print(\"Precision:\", prec)\n",
        "print(\"Recall   :\", rec)\n",
        "print(\"F1 Score :\", f1)\n"
      ],
      "metadata": {
        "id": "yAGIMAVFa2FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_preds=pipeline.predict(test)\n",
        "final_probs=pipeline.predict_proba(test)"
      ],
      "metadata": {
        "id": "3hPXIuZda3-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 10: FINAL DATA PREPARATION\n",
        "# (NO ROW DROPPED, NO NaNs INTRODUCED)\n",
        "# ==========================================\n",
        "print(\"\\nStep 10: Preparing Final Submission DataFrames...\")\n",
        "# Decode predicted labels\n",
        "decoded_labels = le.inverse_transform(final_preds)\n",
        "# Highest confidence per row\n",
        "highest_probs = np.max(final_probs, axis=1)\n",
        "class_names = le.classes_\n"
      ],
      "metadata": {
        "id": "6qDrV3Eea5kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# SUBMISSION 1: ID + PREDICTED CLASS\n",
        "# ==========================================\n",
        "submission1_df = pd.DataFrame({\n",
        "    'id': test_id,\n",
        "    'fruit_name': decoded_labels\n",
        "})"
      ],
      "metadata": {
        "id": "QXegeP0ya5H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# SUBMISSION 2: ID + ALL CLASS PROBABILITIES log loss jaisa\n",
        "# ==========================================\n",
        "prob_cols = {\n",
        "    f\"Status_{cls}\": final_probs[:, i]\n",
        "    for i, cls in enumerate(class_names)\n",
        "}\n",
        "\n",
        "submission2_df = pd.DataFrame(prob_cols)\n",
        "submission2_df.insert(0, 'id', test_id)\n"
      ],
      "metadata": {
        "id": "fp9Wv_U9a8EV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# SUBMISSION 3: ID + CLASS + CONFIDENCE random sa kuch to hai\n",
        "# ==========================================\n",
        "submission3_df = pd.DataFrame({\n",
        "    'id': test_id,\n",
        "    'Predicted_Class': decoded_labels,\n",
        "    'Confidence_Score': highest_probs\n",
        "})\n"
      ],
      "metadata": {
        "id": "QnfEY96wa-F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 11: EXPORT FILES 1- for prediction\n",
        "# 2- log loss styled\n",
        "# 3- just all the probablity in one line\n",
        "# ==========================================\n",
        "\n",
        "submission1_df.to_csv(\"submission1.csv\", index=False)\n",
        "submission2_df.to_csv(\"submission2.csv\", index=False)\n",
        "submission3_df.to_csv(\"submission3.csv\", index=False)\n",
        "print(\"All submissions are generated\")\n"
      ],
      "metadata": {
        "id": "sFHokD7kbA7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1. Amazon uses a deep learning model for product recommendations. Performance improves as more data is added. Why is deep learning suitable here?\n",
        "# Answer: Deep learning can automatically learn complex patterns from large data\n",
        "\n",
        "# Q2. A deep neural network trains well but performs poorly on unseen data. What is the MOST likely issue?\n",
        "# Answer: Overfitting due to high model complexity\n",
        "\n",
        "# Q3. Why are ReLU activations commonly used in deep networks?\n",
        "# Answer: They reduce vanishing gradient problems\n",
        "\n",
        "# Q4. Amazon uses reinforcement learning to optimise warehouse robot paths. What represents the reward?\n",
        "# Answer: Time or energy saved after reaching the destination efficiently\n",
        "\n",
        "# Q5. In reinforcement learning, why is exploration important?\n",
        "# Answer: To discover better actions that may give higher long-term rewards\n",
        "\n",
        "# Q6. An RL agent always chooses the same action even when it‚Äôs not optimal. What is the MOST likely cause?\n",
        "# Answer: Insufficient exploration (agent stuck exploiting)\n",
        "\n",
        "# Q7. Amazon uses computer vision to detect damaged packages from images. Which deep learning model is MOST suitable?\n",
        "# Answer: Convolutional Neural Network (CNN)\n",
        "\n",
        "# Q8. Why are convolution layers effective for image processing?\n",
        "# Answer: They detect local patterns like edges and textures\n",
        "\n",
        "# Q9. A CV model performs well on training images but fails on real warehouse images with different lighting. What is the MAIN issue?\n",
        "# Answer: Overfitting to training conditions (poor generalization)\n",
        "\n",
        "# Q10. Amazon forecasts daily product demand using historical sales data. Why are holidays and promotions important features?\n",
        "# Answer: They explain sudden demand spikes\n",
        "\n",
        "# Q11. Demand forecasts work well normally but fail during major sales events like Prime Day. Why?\n",
        "# Answer: Rare events not well represented in training data\n",
        "\n",
        "# Q12. Why does increasing K generally reduce variance but increase bias?\n",
        "# Answer: Predictions rely on broader neighborhood averaging\n",
        "\n",
        "# Q13. Why is forecasting demand for new products difficult?\n",
        "# Answer: No historical sales data (cold start problem)\n",
        "\n",
        "# Q14. Amazon wants to reduce stockouts. Which forecasting error is more dangerous?\n",
        "# Answer: Large under-forecast causing lost sales\n",
        "\n",
        "# Q15. Why are deep learning models sometimes avoided for simple forecasting tasks?\n",
        "# Answer: They may be unnecessarily complex for simple patterns\n",
        "\n",
        "# Q16. Adding a new feature reduces training error but increases test error. What does this indicate?\n",
        "# Answer: Overfitting due to increased variance\n",
        "\n",
        "# Q17. Which situation can break a supervised model in production even if accuracy was high during testing?\n",
        "# Answer: Concept drift over time\n",
        "\n",
        "# Q18. Which combination BEST matches the task?\n",
        "# Answer: CNN for package damage detection, reinforcement learning for warehouse optimization, and deep learning for demand patterns\n",
        "\n",
        "# Q19. A demand model shows excellent accuracy but is rejected by operations. Why?\n",
        "# Answer: Predictions violate supply chain constraints (storage, delivery limits)\n",
        "\n",
        "# Q20. After adding more training data, a deep demand forecasting model becomes unstable in production. What is the MOST likely deep-learning‚Äìspecific root cause?\n",
        "# Answer: Batch normalization statistics differ between training and inference due to non-stationary data"
      ],
      "metadata": {
        "id": "g3wgye-Ngwuy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}